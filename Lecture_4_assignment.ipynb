{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wctffq6RMEyG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### I have tried coding in OOPS way.This can be converted into modular coding. However there is so much improvement can be done.\n",
        "\n",
        "- Write comment in every methods\n",
        "- There is so much redundant code that needs to be optimized\n",
        "- add customize exception handling\n",
        "- Add logging moodule"
      ],
      "metadata": {
        "id": "cmPVPKwDCWWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "id": "K9dTK-MVRkK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e1052fc-a7cd-46f7-8068-f95039624baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rvz1WaUw6R6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123d2cd6-edf3-481c-e25c-4cf2cb5581b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmetizer = WordNetLemmatizer()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import emoji\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "BOW = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\")\n",
        "print(type(data) == pd.DataFrame)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKKn2NbLxdm0",
        "outputId": "fb79be20-f969-43a6-fc0b-a700ac73e2ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data[\"review\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeCnkSwoykOZ",
        "outputId": "db18cdbb-86f1-49bb-ccf1-7805b184faf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Slang Dictionary"
      ],
      "metadata": {
        "id": "wctffq6RMEyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slang_dict = {\n",
        "  \"AFAIK\": \"As Far As I Know\",\n",
        "  \"AFK\": \"Away From Keyboard\",\n",
        "  \"ASAP\": \"As Soon As Possible\",\n",
        "  \"ATK\": \"At The Keyboard\",\n",
        "  \"ATM\": \"At The Moment\",\n",
        "  \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
        "  \"BAK\": \"Back At Keyboard\",\n",
        "  \"BBL\": \"Be Back Later\",\n",
        "  \"BBS\": \"Be Back Soon\",\n",
        "  \"BFN\": \"Bye For Now\",\n",
        "  \"B4N\": \"Bye For Now\",\n",
        "  \"BRB\": \"Be Right Back\",\n",
        "  \"BRT\": \"Be Right There\",\n",
        "  \"BTW\": \"By The Way\",\n",
        "  \"B4\": \"Before\",\n",
        "  \"B4N\": \"Bye For Now\",\n",
        "  \"CU\": \"See You\",\n",
        "  \"CUL8R\": \"See You Later\",\n",
        "  \"CYA\": \"See You\",\n",
        "  \"FAQ\": \"Frequently Asked Questions\",\n",
        "  \"FC\": \"Fingers Crossed\",\n",
        "  \"FWIW\": \"For What It's Worth\",\n",
        "  \"FYI\": \"For Your Information\",\n",
        "  \"GAL\": \"Get A Life\",\n",
        "  \"GG\": \"Good Game\",\n",
        "  \"GN\": \"Good Night\",\n",
        "  \"GMTA\": \"Great Minds Think Alike\",\n",
        "  \"GR8\": \"Great!\",\n",
        "  \"G9\": \"Genius\",\n",
        "  \"IC\": \"I See\",\n",
        "  \"ICQ\": \"I Seek you (also a chat program)\",\n",
        "  \"ILU\": \"I Love You\",\n",
        "  \"IMHO\": \"In My Honest/Humble Opinion\",\n",
        "  \"IMO\": \"In My Opinion\",\n",
        "  \"IOW\": \"In Other Words\",\n",
        "  \"IRL\": \"In Real Life\",\n",
        "  \"KISS\": \"Keep It Simple, Stupid\",\n",
        "  \"LDR\": \"Long Distance Relationship\",\n",
        "  \"LMAO\": \"Laugh My A.. Off\",\n",
        "  \"LOL\": \"Laughing Out Loud\",\n",
        "  \"LTNS\": \"Long Time No See\",\n",
        "  \"L8R\": \"Later\",\n",
        "  \"MTE\": \"My Thoughts Exactly\",\n",
        "  \"M8\": \"Mate\",\n",
        "  \"NRN\": \"No Reply Necessary\",\n",
        "  \"OIC\": \"Oh I See\",\n",
        "  \"PITA\": \"Pain In The A..\",\n",
        "  \"PRT\": \"Party\",\n",
        "  \"PRW\": \"Parents Are Watching\",\n",
        "  \"QPSA\": \"Que Pasa?\",\n",
        "  \"ROFL\": \"Rolling On The Floor Laughing\",\n",
        "  \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
        "  \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
        "  \"SK8\": \"Skate\",\n",
        "  \"STATS\": \"Your sex and age\",\n",
        "  \"ASL\": \"Age, Sex, Location\",\n",
        "  \"THX\": \"Thank You\",\n",
        "  \"TTFN\": \"Ta-Ta For Now!\",\n",
        "  \"TTYL\": \"Talk To You Later\",\n",
        "  \"U\": \"You\",\n",
        "  \"U2\": \"You Too\",\n",
        "  \"U4E\": \"Yours For Ever\",\n",
        "  \"WB\": \"Welcome Back\",\n",
        "  \"WTF\": \"What The F...\",\n",
        "  \"WTG\": \"Way To Go!\",\n",
        "  \"WUF\": \"Where Are You From?\",\n",
        "  \"W8\": \"Wait...\",\n",
        "  \"7K\": \"Sick:-D Laughter\",\n",
        "  \"TFW\": \"That feeling when\",\n",
        "  \"MFW\": \"My face when\",\n",
        "  \"MRW\": \"My reaction when\",\n",
        "  \"IFYP\": \"I feel your pain\",\n",
        "  \"LOL\": \"Laughing out loud\",\n",
        "  \"TNTL\": \"Trying not to laugh\",\n",
        "  \"JK\": \"Just kidding\",\n",
        "  \"IDC\": \"I don’t care\",\n",
        "  \"ILY\": \"I love you\",\n",
        "  \"IMU\": \"I miss you\",\n",
        "  \"ADIH\": \"Another day in hell\",\n",
        "  \"IDC\": \"I don’t care\",\n",
        "  \"ZZZ\": \"Sleeping, bored, tired\",\n",
        "  \"WYWH\": \"Wish you were here\",\n",
        "  \"TIME\": \"Tears in my eyes\",\n",
        "  \"BAE\": \"Before anyone else\",\n",
        "  \"FIMH\": \"Forever in my heart\",\n",
        "  \"BSAAW\": \"Big smile and a wink\",\n",
        "  \"BWL\": \"Bursting with laughter\",\n",
        "  \"LMAO\": \"Laughing my a** off\",\n",
        "  \"BFF\": \"Best friends forever\",\n",
        "  \"CSL\": \"Can’t stop laughing\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "tFBFEHZuL_YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding starts"
      ],
      "metadata": {
        "id": "BOW1-0mwMISI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextTranformerSeries:\n",
        "  def __init__(self, data):\n",
        "    self.regex_types = {\n",
        "      \"tags_removal\": re.compile('<.*?>'),\n",
        "      \"url_removal\": re.compile(r'https?://\\S+|www\\.\\S+'),\n",
        "      \"punctuation_removal\": re.compile(r'[^\\w\\s]'),\n",
        "      \"extra_space_removal\": re.compile(r'\\s+'),\n",
        "      \"emoji_removal\": re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    }\n",
        "\n",
        "  def convert_to_lowercase_series(self,data):\n",
        "    str_data = len((data == str) == True) == len(data)\n",
        "    if not str_data:\n",
        "      cleanup_data = input(\"Your data contains some other value than string, Do you want to drop it and have only string(y/n):\")\n",
        "      if cleanup_data == 'y':\n",
        "        data = data.apply(lambda x: isinstance(x, str))\n",
        "        print(\"All non string values has been removed\")\n",
        "      else:\n",
        "        print(\"Please clean up non string value as you want and use method again\")\n",
        "        return data\n",
        "    return data.str.lower()\n",
        "\n",
        "  def series_operation_based_on_regex(self,data,type):\n",
        "    if type(data) == str:\n",
        "      data =  self.regex_types[type].sub(\"\",data)\n",
        "    else:\n",
        "      data = data.apply(lambda x: self.regex_types[type].sub(\"\",x))\n",
        "    return data\n",
        "\n",
        "  def spell_correction_series(self,data):\n",
        "    return data.apply(lambda x: TextBlob(x).correct().string)\n",
        "\n",
        "  def convert_slangs(self,text):\n",
        "    txt = []\n",
        "    for word in text.split():\n",
        "      if word.upper() in slang_dict.keys():\n",
        "        txt.append(slang_dict[word])\n",
        "      else:\n",
        "        txt.append(word)\n",
        "    return \" \".join(txt)\n",
        "\n",
        "  def replace_slangs(self,data):\n",
        "    return data.apply(lambda x: self.convert_slangs(x))\n",
        "\n",
        "  def remove_stopwords_from_series(self,data):\n",
        "    return data.apply(lambda x: ' '.join([word for word in nltk.word_tokenize(x) if word.lower() not in stopwords.words('english')]))\n",
        "\n",
        "  def demojize_emoji_on_series(self,data):\n",
        "    return data.apply(lambda x: emoji.demojize(x))\n",
        "\n",
        "  def tokenize_series_based_on_seperator(self,data,sep):\n",
        "    return data.apply(lambda x: x.split(sep))\n",
        "\n",
        "  def tokenize_series_using_regex(self,data):\n",
        "    if type(data) == str:\n",
        "      data =  re.findall(\"[\\w]+\",data)\n",
        "    else:\n",
        "     data = data.apply(lambda x: re.findall(\"[\\w]+\",x))\n",
        "    return data\n",
        "\n",
        "  def tokenize_series_based_on_nltk(self,data,method):\n",
        "    if method == 'word':\n",
        "      data = data.apply(lambda x: word_tokenize(x))\n",
        "    elif method == 'sentence':\n",
        "      data = data.apply(lambda x: sent_tokenize(x))\n",
        "    return data\n",
        "\n",
        "  def remove_stopwords_from_series(self,data):\n",
        "    return data.apply(lambda x: \" \".join([s for s in x.split() if s not in stopwords.words(\"english\")]))\n",
        "\n",
        "  def stemmatize_series(self,data):\n",
        "    return data.apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))\n",
        "\n",
        "  def lemmatize_series(self,data):\n",
        "    return data.apply(lambda x: \" \".join([lemmetizer.lemmatize(word) for word in x.split()]))\n",
        "\n",
        "  def bag_of_words_series(self, data):\n",
        "    return data.apply()\n",
        "\n",
        "  def encode_series_based_on_BOW(self,data):\n",
        "    document = BOW.fit_transform(data)\n",
        "    if type(data) == str:\n",
        "      return document[0].toarray()\n",
        "    else:\n",
        "      data = document.apply(lambda x: x.toarray())\n",
        "    return data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V5ZqUOGgxC7X"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextTranformer(TextTranformerSeries):\n",
        "  def __init__(self,data):\n",
        "    super().__init__()\n",
        "    self.data =  data\n",
        "    if type(data) == pd.DataFrame:\n",
        "      self.categorical_columns = self.data.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "  def convert_to_lower(self):\n",
        "    if type(self.data) == pd.Series:\n",
        "      self.data = super().convert_to_lower(self.data)\n",
        "    elif type(self.data) == pd.DataFrame:\n",
        "      self.data[self.categorical_columns] = self.data[self.categorical_columns].apply(lambda x: self.convert_to_lowercase_series(x))\n",
        "    else:\n",
        "      self.data = self.data.lower()\n",
        "\n",
        "  def operation_based_on_regex(self,type):\n",
        "    return self.series_operation_based_on_regex(self,self.data,type)\n",
        "\n",
        "  def remove_html_tags(self):\n",
        "    if type(self.data) in [pd.Series, str]:\n",
        "      self.data = self.operation_based_on_regex(self.data, type=\"tags_removal\")\n",
        "    else:\n",
        "      self.data[self.categorical_columns] = self.data[self.categorical_columns].apply(lambda x: self.operation_based_on_regex(x, type=\"tags_removal\"))\n",
        "\n",
        "  def remove_url(self):\n",
        "    if type(self.data) in [pd.Series, str]:\n",
        "      self.data = self.operation_based_on_regex(self.data, type=\"url_removal\")\n",
        "    else:\n",
        "      self.data[self.categorical_columns] = self.data[self.categorical_columns].apply(lambda x: self.operation_based_on_regex(x, type=\"url_removal\"))\n",
        "\n",
        "  def remove_punctuation(self):\n",
        "    if type(self.data) in [pd.Series, str]:\n",
        "      self.data = self.operation_based_on_regex(self.data, type=\"punctuation_removal\")\n",
        "    else:\n",
        "      self.data[self.categorical_columns] = self.data[self.categorical_columns].apply(lambda x: self.operation_based_on_regex(x, type=\"punctuation_removal\"))\n",
        "\n",
        "  def remove_extra_spaces(self):\n",
        "    if type(self.data) in [pd.Series, str]:\n",
        "      self.data = self.operation_based_on_regex(self.data, type=\"extra_space_removal\")\n",
        "    else:\n",
        "      self.data[self.categorical_columns] = self.data[self.categorical_columns].apply(lambda x: self.operation_based_on_regex(x, type=\"extra_space_removal\"))\n",
        "\n",
        "  def spell_correction(self):\n",
        "    if type(self.data) == str:\n",
        "      self.data = TextBlob(self.data).correct().string\n",
        "    else:\n",
        "      self.data[self.categorical_columns] = self.data[self.categorical_columns].apply(lambda x: self.spell_correction_series(x))\n",
        "\n",
        "  def demojize(self):\n",
        "    if type(self.data) in [pd.Series, str]:\n",
        "      self.data = self.demojize_emoji_on_series(self.data)\n",
        "    else:\n",
        "      self.data[self.categorical_columns] = self.data[self.categorical_columns].apply(lambda x: self.demojize_emoji_on_series(x))\\\n",
        "\n",
        "  def remove_emoji(self):\n",
        "    if type(self.data) in [pd.Series, str]:\n",
        "      self.data = self.operation_based_on_regex(self.data, type=\"emoji_removal\")\n",
        "    else:\n",
        "      self.data[self.categorical_columns] = self.data[self.categorical_columns].apply(lambda x: self.operation_based_on_regex(x, type=\"emoji_removal\"))\n",
        "\n",
        "  def tokenize_based_on_seperator(self, sep):\n",
        "    if type(self.data) == str:\n",
        "      self.data = self.data.split(sep)\n",
        "    elif type(self.data) == pd.Series:\n",
        "      self.data = self.tokenize_series_based_on_seperator(self.data,sep)\n",
        "    else:\n",
        "      self.data[self.categorical_columns] = self.data[self.categorical_columns].apply(lambda x: self.tokenize_series_based_on_seperator(x,sep))\n",
        "\n",
        "  def tokenize_based_on_regex(self):\n",
        "    if type(self.data) in [pd.Series, str]:\n",
        "      self.data = self.tokenize_series_using_regex(self.data)\n",
        "    else:\n",
        "      self.data[self.categorical_columns] = self.data[self.categorical_columns].apply(lambda x: self.tokenize_series_using_regex(x))\n",
        "\n",
        "  def remove_stopwords(self):\n",
        "    if type(self.data) == str:\n",
        "      self.data = \" \".join([s for s in self.data.split() if s not in stopwords.words(\"english\")])\n",
        "    elif type(self.data) == pd.Series:\n",
        "      self.data = self.remove_stopwords_from_series(self.data)\n",
        "    else:\n",
        "      self.data[self.categorical_columns] = self.data[self.categorical_columns].apply(lambda x: self.remove_stopwords_from_series(x))\n",
        "\n",
        "  def tokenize_based_on_nltk(self,method):\n",
        "    if type(self.data) == str and method == \"word\":\n",
        "      self.data = word_tokenize(self.data)\n",
        "    elif type(self.data) == str and method == \"sentence\":\n",
        "      self.data = sent_tokenize(self.data)\n",
        "    elif type(self.data) == pd.Series:\n",
        "      self.data = self.tokenize_series_based_on_nltk(self.data,method)\n",
        "    else:\n",
        "      self.data[self.categorical_columns] = self.data[self.categorical_columns].apply(lambda x: self.tokenize_series_based_on_nltk(self.data,method))\n",
        "\n",
        "  def stemmatization(self):\n",
        "    if type(self.data) == str:\n",
        "      self.data = \" \".join([stemmer.stem(word) for word in self.data.split()])\n",
        "    elif type(self.data) == pd.Series:\n",
        "      self.data = self.stemmatize_series(self.data)\n",
        "    else:\n",
        "      self.data[self.categorical_columns] = self.data[self.categorical_columns].apply(lambda x: self.stemmatize_series(self.data))\n",
        "\n",
        "  def lemmatization(self):\n",
        "    if type(self.data) == str:\n",
        "      self.data = \" \".join([lemmetizer.lemmatize(word) for word in self.data.split()])\n",
        "    elif type(self.data) == pd.Series:\n",
        "      self.data = self.lemmetizer.lemmatize(self.data)\n",
        "    else:\n",
        "      self.data[self.categorical_columns] = self.data[self.categorical_columns].apply(lambda x: self.lemmetizer.lemmatize(self.data))\n",
        "\n",
        "  def encode_based_on_BOW(self):\n",
        "    if type(self.data) in [pd.Series, str]:\n",
        "      self.data = self.encode_series_based_on_BOW(self.data)\n",
        "    else:\n",
        "      self.data[self.categorical_columns] = self.data[self.categorical_columns].apply(lambda x: self.encode_series_based_on_BOW(x))\n"
      ],
      "metadata": {
        "id": "tmWf12Vc00-j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transformer = TextTranformer(df)\n",
        "transformer.tokenize_based_on_nltk(\"word\")\n",
        "# transformer.spell_correction()\n",
        "transformer.data\n"
      ],
      "metadata": {
        "id": "xzOXfD1rzkqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4a1lzq2KuTMm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}